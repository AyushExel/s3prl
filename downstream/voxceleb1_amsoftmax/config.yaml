
# this voxceleb1 is doing speaker classification task!
runner:
  total_steps: 100000
  gradient_clipping: 1
  gradient_accumulate_steps: 1

  log_step: 500
  eval_step: 4000
  save_step: 4000
  max_keep: 4
  eval_dataloaders: 
    - dev
    - test
  
optimizer: 
  name: AdamW
  lr: 1.0e-4

# # comment the whole scheduler config block to disable learning rate scheduling
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 5000

#comment the whole augmentation config block to disable augmentation when training!!
augmentation:
  type: 1 # 1: train aug 2: use pretrain aug module 3:previous specaug 4: no aug(only means no aug on spectrogram)

  specaug: # arg meaning please refer to src/audio.py: class Augment # only valid if type==3
    T: 40
    num_masks: 1
    replace_with_zero: False
    F: 27

  trainable_aug: # arg meaning please refer to src/augmentation.py: class TrainableAugment # only valid if type==1,2
    model:
      T_num_masks: 1
      F_num_masks: 1
      # noise_dim: None
      dim: [3, 3]
      replace_with_zero: False
      width_init_bias: -3.
    optimizer:
      optimizer: 'AdamW'
      lr: 1.0e-4
      amsgrad: True
  
  ckpt_path: your_aug_model_path.ckpt
    

downstream_expert: 
  datarc:
    file_path: /home/pohan/data/librispeech/vox1_dev/wav
    meta_data: /home/pohan/data/librispeech/vox1_test/veri_test_class.txt
    num_workers: 8
    train_batch_size: 16
    dev_batch_size: 16
    eval_batch_size: 1
    max_timestep: 80000


  modelrc:
    module:
      TransformerEncoder
    hparams:
      hidden_size: 768                                      # Size of the encoder layers and the pooler layer.
      num_hidden_layers: 3                                  # Number of hidden layers in the Transformer encoder.
      num_attention_heads: 12                               # Number of attention heads for each attention layer in the Transformer encoder.
      intermediate_size: 3072                               # The size of the "intermediate" (i.e., feed-forward) layer in the Transformer encoder.
      hidden_act: "gelu"                                    # The non-linear activation function (function or string) in the encoder and pooler. If string, "gelu", "relu" and "swish" are supported.
      hidden_dropout_prob: 0.1                              # The dropout probabilitiy for all fully connected layers in the embeddings, encoder, and pooler.
      attention_probs_dropout_prob: 0.1                     # The dropout ratio for the attention probabilities.
      initializer_range: 0.02                               # The sttdev of the truncated_normal_initializer for initializing all weight matrices.
      layer_norm_eps: 1.0e-12                              # The epsilon used by LayerNorm.
      share_layer: False                                    # Share layer weights
      max_input_length: 0                                   # maximum input length (0 for no restriction)
      pre_layer_norm: False                                 # apply the pre layer normalization technique introduced in: https://arxiv.org/abs/2002.04745
    input_dim: 768
    agg_module: SAP
